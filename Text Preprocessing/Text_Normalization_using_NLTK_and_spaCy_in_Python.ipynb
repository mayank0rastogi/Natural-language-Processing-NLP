{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Text Normalization using NLTK and spaCy in Python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk5Msc8nQeaX"
      },
      "source": [
        "A note here – we need to perform tokenization before removing any stopwords. I encourage you to go through my article below on the different methods to perform tokenization:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhgfHZVGQpwl"
      },
      "source": [
        "Removing stopwords is not a definite or say any fixed  rule in NLP. It depends upon the task that we are working on. For tasks like text classification, where the text is to be classified into different categories, stopwords are removed or excluded from the given text so that more focus can be given to those words which define the meaning of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED6AjBT9Q8RK"
      },
      "source": [
        "However, in tasks like machine translation and text summarization, removing stopwords is not advisable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIHhBgmIRAzB"
      },
      "source": [
        "Important reasons of removing stopwords:\n",
        "\n",
        "* On removing stopwords, dataset size decreases and the time to train the model also decreases\n",
        "\n",
        "* Removing stopwords can potentially help improve the performance as there are fewer and only meaningful tokens left. Thus, it could increase classification accuracy\n",
        "\n",
        "* Even search engines like Google remove stopwords for fast and relevant retrieval of data from the database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSpbHs17RSfa"
      },
      "source": [
        "The Questions Arises When i shoukd remove the stop words and when not?\n",
        "\n",
        "We can remove Stop Words In following Case:\n",
        "* Text Classification\n",
        "* Spam Filtering\n",
        "* Language Classification\n",
        "* Genre Classification\n",
        "* Caption Generation\n",
        "* Auto-Tag Generation\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxWurFftRz85"
      },
      "source": [
        "avoid removing stopwords Removal\n",
        "* Machine Translation\n",
        "* Language Modeling\n",
        "* Text Summarization\n",
        "* Question-Answering problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ClL96qTSFne"
      },
      "source": [
        " NLTK has provide us a list of stopwords stored in **16 different languages.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKgIj98WQpa6",
        "outputId": "c41050b4-1396-4909-aa63-53b4092dd8b0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('punkt')\n",
        "print(len(set(stopwords.words('english'))))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMiCTLA7SQvz",
        "outputId": "f5b694a0-f046-412c-b033-44208b5b97fa"
      },
      "source": [
        "# sample sentence\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "# set of stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# tokens of words  \n",
        "word_tokens = word_tokenize(text) \n",
        "    \n",
        "filtered_sent = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sent.append(w) \n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\nOriginal Sentence \\n\\n\")\n",
        "print(\" \".join(word_tokens)) \n",
        "\n",
        "print(\"\\n\\nFiltered Sentence \\n\\n\")\n",
        "print(\" \".join(filtered_sent)) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Original Sentence \n",
            "\n",
            "\n",
            "He determined to drop his litigation with the monastry , and relinguish his claims to the wood-cuting and fishery rihgts at once . He was the more ready to do this becuase the rights had become much less valuable , and he had indeed the vaguest idea where the wood and river in question were .\n",
            "\n",
            "\n",
            "Filtered Sentence \n",
            "\n",
            "\n",
            "He determined drop litigation monastry , relinguish claims wood-cuting fishery rihgts . He ready becuase rights become much less valuable , indeed vaguest idea wood river question .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMY-vR9XAPHS"
      },
      "source": [
        "**Stopword Removal using spaCy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IzCJN00Ajh0"
      },
      "source": [
        "**It has a list of its own stopwords that can be imported as STOP_WORDS from the spacy.lang.en.stop_words class.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYR3dsjEWYvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3cb407-c3f9-46ac-8ae2-1ae4b7da6ea6"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"Apartment Therapy is a blog focusing on interior design. It was launched by Maxwell Ryan in 2001. Ryan is an interior designer who turned to blogging (using the moniker “the apartment therapist”). The blog has reached 20 million followers and has expanded into a full-scale media company.\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "print(my_doc)\n",
        "print(\"_______________________________________________\")\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "\n",
        "print(token_list)\n",
        "print(\"__________________________________________________\")\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[] \n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word) \n",
        "#print(token_list)\n",
        "print(\"-------------------------------------------------------------\")\n",
        "print(filtered_sentence)   "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apartment Therapy is a blog focusing on interior design. It was launched by Maxwell Ryan in 2001. Ryan is an interior designer who turned to blogging (using the moniker “the apartment therapist”). The blog has reached 20 million followers and has expanded into a full-scale media company.\n",
            "_______________________________________________\n",
            "['Apartment', 'Therapy', 'is', 'a', 'blog', 'focusing', 'on', 'interior', 'design', '.', 'It', 'was', 'launched', 'by', 'Maxwell', 'Ryan', 'in', '2001', '.', 'Ryan', 'is', 'an', 'interior', 'designer', 'who', 'turned', 'to', 'blogging', '(', 'using', 'the', 'moniker', '“', 'the', 'apartment', 'therapist', '”', ')', '.', 'The', 'blog', 'has', 'reached', '20', 'million', 'followers', 'and', 'has', 'expanded', 'into', 'a', 'full', '-', 'scale', 'media', 'company', '.']\n",
            "__________________________________________________\n",
            "-------------------------------------------------------------\n",
            "['Apartment', 'Therapy', 'blog', 'focusing', 'interior', 'design', '.', 'launched', 'Maxwell', 'Ryan', '2001', '.', 'Ryan', 'interior', 'designer', 'turned', 'blogging', '(', 'moniker', '“', 'apartment', 'therapist', '”', ')', '.', 'blog', 'reached', '20', 'million', 'followers', 'expanded', '-', 'scale', 'media', 'company', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UynkXFY5BklI"
      },
      "source": [
        "**Stopword Removal using Gensim**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loGvFkESBprm"
      },
      "source": [
        "We can easily import the remove_stopwords method from the class gensim.parsing.preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFgo0gjYBOQn",
        "outputId": "2527fea4-7eee-4f1d-c811-16e26c262186"
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "t=\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, \n",
        "and he had indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "# pass the sentence in the remove_stopwords function\n",
        "result = remove_stopwords(t)\n",
        "\n",
        "print('\\n\\n Filtered Sentence \\n\\n')\n",
        "print(result) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Filtered Sentence \n",
            "\n",
            "\n",
            "He determined drop litigation monastry, relinguish claims wood-cuting fishery rihgts once. He ready becuase rights valuable, vaguest idea wood river question were.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LSfrEQTCDBa"
      },
      "source": [
        "**Text Normalization**\n",
        "\n",
        "In any natural language, words can be written or spoken in more than one form depending on the situation. That’s what makes the language such a thrilling part of our lives, right? \n",
        "\n",
        "For example:\n",
        "\n",
        "* Lisa ate the food and washed the dishes.\n",
        "* They were eating noodles at a cafe.\n",
        "* Don’t you want to eat before we leave?\n",
        "* We have just eaten our breakfast.\n",
        "* It also eats fruit and vegetables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yihmo3LiCcGc"
      },
      "source": [
        "In above examples, we can see that the word eat has been used as many forms. so, it is easy to understand that eating is the activity here. So it doesn’t really matter to us whether it is ‘ate’, ‘eat’, or ‘eaten’ – we know what is going on.\n",
        "\n",
        "Unfortunately, that is not the case with machines. They treat these words differently. Therefore, we need to normalize them to their root word, which is “eat” in our example.\n",
        "\n",
        "Hence, text normalization is a process of transforming a word into a single canonical form. This can be done by two processes, stemming and lemmatization. Let’s understand what they are in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfY7vsrkC0Qk"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJKHbRULC1xQ"
      },
      "source": [
        "Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word\n",
        "stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fgZpW9kC_xd"
      },
      "source": [
        "**Lemmatization**\n",
        "\n",
        "Lemmatization, works in an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGodToqJDW9V"
      },
      "source": [
        "Why do we need to Perform Stemming or Lemmatization?\n",
        "Let’s consider the following two sentences:\n",
        "\n",
        "\n",
        "We can easily state that  the sentences are conveying the same meaning, that is, some activity in the past. A machine will treat both sentences differently. Thus, to make the text understandable for the machine, we need to perform stemming or lemmatization.\n",
        "\n",
        "Another benefit of text normalization is that it reduces the number of unique words in the text data. This helps in bringing down the training time of the machine learning model (and don’t we all want that?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_jXK5JdD-xj"
      },
      "source": [
        "**Methods to Perform Text Normalization**\n",
        "\n",
        "\n",
        "Text Normalization using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipXMlx5aEKt-"
      },
      "source": [
        "The NLTK library has a lot of amazing methods to perform different steps of data preprocessing. There are methods like PorterStemmer() and WordNetLemmatizer() to perform stemming and lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a1x_worEtKf"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YKSHCOBB1-v",
        "outputId": "1e184ac7-71c1-425c-8d48-c3dabf0d5d43"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "set(stopwords.words('english'))\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(text) \n",
        "    \n",
        "filtered_sentence = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "\n",
        "Stem_words = []\n",
        "ps =PorterStemmer()\n",
        "for w in filtered_sentence:\n",
        "    rootWord=ps.stem(w)\n",
        "    Stem_words.append(rootWord)\n",
        "print(filtered_sentence)\n",
        "print(Stem_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
            "['He', 'determin', 'drop', 'litig', 'monastri', ',', 'relinguish', 'claim', 'wood-cut', 'fisheri', 'rihgt', '.', 'He', 'readi', 'becuas', 'right', 'becom', 'much', 'less', 'valuabl', ',', 'inde', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnA9867DEu8K"
      },
      "source": [
        "**Lemmtization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwLCVhYbFBsL",
        "outputId": "6da053a9-e3d0-4079-8812-e54ca5e6aa23"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvRNeGucEr44",
        "outputId": "91b63680-94f5-41bd-cbb8-d275756894e0"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "set(stopwords.words('english'))\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(text) \n",
        "    \n",
        "filtered_sentence = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "print(filtered_sentence) \n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "lemma_word = []\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "for w in filtered_sentence:\n",
        "  #Lemmatization is done on the basis of part-of-speech tagging (POS tagging)\n",
        "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
        "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "    lemma_word.append(word3)\n",
        "print(lemma_word)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
            "--------------------------------------------------------------------------------------------\n",
            "['He', 'determine', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claim', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'right', 'become', 'much', 'le', 'valuable', ',', 'indeed', 'vague', 'idea', 'wood', 'river', 'question', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOqdD1bdFkli"
      },
      "source": [
        "**Text Normalization using spaCy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz_fNIMEGR77"
      },
      "source": [
        "**It only supports lemmetization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z49fE7ZAE-aK"
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "doc = nlp(u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\")\n",
        "\n",
        "lemma_word1 = [] \n",
        "for token in doc:\n",
        "    lemma_word1.append(token.lemma_)\n",
        "q=lemma_word1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSMY1VQmF4yG",
        "outputId": "3ee79447-b474-4974-8ae2-b2cbddd77407"
      },
      "source": [
        "q"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-PRON-',\n",
              " 'determine',\n",
              " 'to',\n",
              " 'drop',\n",
              " '-PRON-',\n",
              " 'litigation',\n",
              " 'with',\n",
              " 'the',\n",
              " 'monastry',\n",
              " ',',\n",
              " 'and',\n",
              " 'relinguish',\n",
              " '-PRON-',\n",
              " 'claim',\n",
              " 'to',\n",
              " 'the',\n",
              " 'wood',\n",
              " '-',\n",
              " 'cut',\n",
              " 'and',\n",
              " '\\n',\n",
              " 'fishery',\n",
              " 'rihgts',\n",
              " 'at',\n",
              " 'once',\n",
              " '.',\n",
              " '-PRON-',\n",
              " 'be',\n",
              " 'the',\n",
              " 'more',\n",
              " 'ready',\n",
              " 'to',\n",
              " 'do',\n",
              " 'this',\n",
              " 'becuase',\n",
              " 'the',\n",
              " 'right',\n",
              " 'have',\n",
              " 'become',\n",
              " 'much',\n",
              " 'less',\n",
              " 'valuable',\n",
              " ',',\n",
              " 'and',\n",
              " '-PRON-',\n",
              " 'have',\n",
              " '\\n',\n",
              " 'indeed',\n",
              " 'the',\n",
              " 'vague',\n",
              " 'idea',\n",
              " 'where',\n",
              " 'the',\n",
              " 'wood',\n",
              " 'and',\n",
              " 'river',\n",
              " 'in',\n",
              " 'question',\n",
              " 'be',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JouX4EfUGCpf"
      },
      "source": [
        "Here -PRON- is the notation for pronoun which could easily be removed using regular expressions. The benefit of spaCy is that we do not have to pass any pos parameter to perform lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA3jjtzsGIph"
      },
      "source": [
        "**Text Normalization using TextBlob**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO5HRc28GOnv"
      },
      "source": [
        "**lemmetization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivH_VMExF57l",
        "outputId": "d2e07079-a76a-4081-a11a-b33430ba1a50"
      },
      "source": [
        "# from textblob lib import Word method \n",
        "from textblob import Word \n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "lem = []\n",
        "for i in text.split():\n",
        "    word1 = Word(i).lemmatize(\"n\")\n",
        "    word2 = Word(word1).lemmatize(\"v\")\n",
        "    word3 = Word(word2).lemmatize(\"a\")\n",
        "    lem.append(Word(word3).lemmatize())\n",
        "print(lem)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'determine', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry,', 'and', 'relinguish', 'his', 'claim', 'to', 'the', 'wood-cuting', 'and', 'fishery', 'rihgts', 'at', 'once.', 'He', 'wa', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'right', 'have', 'become', 'much', 'le', 'valuable,', 'and', 'he', 'have', 'indeed', 'the', 'vague', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}