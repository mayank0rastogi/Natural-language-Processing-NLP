{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing(Basics).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwh4SI-_1hJ4",
        "outputId": "be257605-77a9-4066-e9be-023b7928b0c8"
      },
      "source": [
        "import re\n",
        "doc = \"NLP  is an interesting    field.  \"\n",
        "new_doc = re.sub(\"\\s+\",\" \", doc)\n",
        "print(new_doc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLP is an interesting field. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iRK22tps87VB",
        "outputId": "a8c40047-898d-49f6-b11b-36eceddb31e7"
      },
      "source": [
        "new = \"\".join(doc)\n",
        "new"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'NLP  is an interesting    field.  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8gMd8jr8Zh8"
      },
      "source": [
        "**Removing Punctutaions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IyxvMQPE7_GY",
        "outputId": "d4345bf7-dd26-490a-ab80-d3fde0da20a1"
      },
      "source": [
        "t = \"Hello! How are you!! I'm very excited that you're going for a trip to Europe!! Yayy!\"\n",
        "re.sub(\"[^-9A-Za-z ]\", \"\" , t)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello How are you Im very excited that youre going for a trip to Europe Yayy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeWgCB8881nP"
      },
      "source": [
        "**Punctutaion removed using Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_h0f_N6M8xMR",
        "outputId": "7c80a494-bc0a-40ce-e9c2-5cda17ca7ca6"
      },
      "source": [
        "import string\n",
        "t = \"Hello! How are you!! I'm very excited that you're going for a trip to Europe!! Yayy!\"\n",
        "t_clean = \"\".join([i for i in t if i not in string.punctuation])\n",
        "t_clean"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello How are you Im very excited that youre going for a trip to Europe Yayy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47lpJNJT-Bs-"
      },
      "source": [
        "Tokenization: Splitting a sentence into words and creating a list, ie each sentence is a list of words. There are mainly 3 types of tokenizers.\n",
        "\n",
        "word_tokenize: It is a generic tokenizer that separates words and punctuations. An apostrophe is not considered as punctuation here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuQ1DmZK-NoU",
        "outputId": "43c91590-ab28-4ccd-d90d-c5858121ccab"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9ZkUj5T9n4q",
        "outputId": "4951dd3b-5d19-4975-d18e-245247c00544"
      },
      "source": [
        "t = \"Hello! How are you!! I'm very excited that you're going for a trip to Europe!! :d Yayy!;)!\"\n",
        "nltk.tokenize.word_tokenize(t)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'How',\n",
              " 'are',\n",
              " 'you',\n",
              " '!',\n",
              " '!',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'very',\n",
              " 'excited',\n",
              " 'that',\n",
              " 'you',\n",
              " \"'re\",\n",
              " 'going',\n",
              " 'for',\n",
              " 'a',\n",
              " 'trip',\n",
              " 'to',\n",
              " 'Europe',\n",
              " '!',\n",
              " '!',\n",
              " ':',\n",
              " 'd',\n",
              " 'Yayy',\n",
              " '!',\n",
              " ';',\n",
              " ')',\n",
              " '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9GMz3jA-9Kj"
      },
      "source": [
        "TweetTokenizer: This is specifically used while dealing with text data from social media consisting of #,@, emoticons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmLMZEAc-Leb",
        "outputId": "814af65a-d22f-497a-e248-5c331ed3e97e"
      },
      "source": [
        "t = \"Hello! How are you!! I'm very excited that you're going for a trip to @Europe :d!!  #Yayy!\"\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet = TweetTokenizer()\n",
        "tweet.tokenize(t)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'How',\n",
              " 'are',\n",
              " 'you',\n",
              " '!',\n",
              " '!',\n",
              " \"I'm\",\n",
              " 'very',\n",
              " 'excited',\n",
              " 'that',\n",
              " \"you're\",\n",
              " 'going',\n",
              " 'for',\n",
              " 'a',\n",
              " 'trip',\n",
              " 'to',\n",
              " '@Europe',\n",
              " ':d',\n",
              " '!',\n",
              " '!',\n",
              " '#Yayy',\n",
              " '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93rkl5z7_gLS"
      },
      "source": [
        "c. regexp_tokenize: It can be used when we want to separate words of our interests which follows a common pattern like extracting all hashtags from tweets, addresses from tweets, or hyperlinks from the text. In this, you can use the normal regular expression functions to separate the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37go4LYq_Rgy",
        "outputId": "1ca93ea7-90ac-4700-a304-98a9d6c9229c"
      },
      "source": [
        "import re\n",
        "a = 'What are your views related to US elections @nitin'\n",
        "re.split('\\s@', a)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are your views related to US elections', 'nitin']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Jco1msABRm"
      },
      "source": [
        "Removing Stopwords\n",
        "Stopwords include: I, he, she, and, but, was were, being, have, etc, which do not add meaning to the data. So these words must be removed which helps to reduce the features from our data. **These are removed after tokenizing the text.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7gLcVwFAXJX",
        "outputId": "e812a73c-995c-4a3f-c201-cbaada1e142b"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-iEvc73_mlL",
        "outputId": "9f77ca73-9cea-4403-e556-a8c8bc94d0dc"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "t = \"Hello! How are you!! I'm very excited that you're going for a trip to Europe!! Yayy!\"\n",
        "t_n = \"\".join([i for i in t if i not in string.punctuation])\n",
        "print(t_n)\n",
        "words = nltk.tokenize.word_tokenize(t_n)\n",
        "print(words)\n",
        "words_n = [i for i in words if i not in stopwords]\n",
        "print(words_n)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello How are you Im very excited that youre going for a trip to Europe Yayy\n",
            "['Hello', 'How', 'are', 'you', 'Im', 'very', 'excited', 'that', 'youre', 'going', 'for', 'a', 'trip', 'to', 'Europe', 'Yayy']\n",
            "['Hello', 'How', 'Im', 'excited', 'youre', 'going', 'trip', 'Europe', 'Yayy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7YpuV2SAUOg",
        "outputId": "ba118521-de54-4b36-9afb-f94edfeca10f"
      },
      "source": [
        "words_n = [i for i in words if i not in stopwords and len(i)>2]\n",
        "print(words_n)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'How', 'excited', 'youre', 'going', 'trip', 'Europe', 'Yayy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEg6U_j6BJDd"
      },
      "source": [
        "**Lemmatization & Stemming**\n",
        "a. Stemming: A technique that takes the word to its root form. It just removes suffixes from the words. The stemmed word might not be part of the dictionary, i.e it will not necessarily give meaning. There are two main types of stemmer- Porter Stemmer and Snow Ball Stemmer(advanced version of Porter Stemmer).\n",
        "\n",
        "CODE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BDewG5LA9O_",
        "outputId": "10059d4e-370b-4c0f-eca2-ef0302b8ebb9"
      },
      "source": [
        "ps = nltk.PorterStemmer()\n",
        "w = [ps.stem(word) for word in words_n]\n",
        "print(w)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'how', 'excit', 'your', 'go', 'trip', 'europ', 'yayi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EQpuvejBa9m",
        "outputId": "a1efade4-2600-4cc9-aa07-076b02ebe968"
      },
      "source": [
        "ss = nltk.SnowballStemmer(language = 'english')\n",
        "w = [ss.stem(word) for word in words_n]\n",
        "print(w)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'how', 'excit', 'your', 'go', 'trip', 'europ', 'yayi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQv1EMn8CEtb",
        "outputId": "331921d7-2197-429d-a427-27c333056ebc"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOxxktJgCKZA"
      },
      "source": [
        "Lemmatization: Takes the word to its root form called Lemma. It helps to bring words to their dictionary form. It is applied to nouns by default. It is more accurate as it uses more informed analysis to create groups of words with similar meanings based on the context, so it is complex and takes more time. This is used where we need to retain the contextual information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6iIVvFnBvzB",
        "outputId": "17a493db-e285-4ee0-da83-05e34840fa0e"
      },
      "source": [
        "wn = nltk.WordNetLemmatizer()\n",
        "w = [wn.lemmatize(word) for word in words_n]\n",
        "print(w)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'How', 'excited', 'youre', 'going', 'trip', 'Europe', 'Yayy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hobfvt0nCCNI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}